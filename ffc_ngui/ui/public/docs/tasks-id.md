### **Summary**

Use this page to visualize and access task data for analysis, create leaderboards, and get recommendations.

### **View**

- Performance Management Toolbar: Use the Page toolbar with the "Refresh", "Profiling Integration", and "Configure" buttons.

- Summary Cards: Get an overview of the last run, lifetime cost, and potential summary savings for the task.

- Overview: Monitor task details and latest run information.

- Runs: Analyze data on task runs. Monitor selected metrics according to the Category selected in the Filter on the runs 
  chart. Review details for each run in the runs table.

- Model Version: View versions of models that were created in task runs.

- Leaderboards: Compare groups of task runs (candidates) that are grouped by hyperparameters and tags to obtain optimal 
  launch parameters.

- Executors: Check executors on which training code was executed. 

### **Actions**

- Configure a Task: Use the "Configure" button to change the default task parameters, set up metrics, and configure a 
  leaderboard template.

- Add a Leaderboard: Easily create a new leaderboard by clicking the "+" button (by default, the leaderboard is based on 
  the leaderboard template). Specify the criteria for grouping runs, qualification protocol, and dataset coverage rules. 
  When a new run is added, the leaderboards are recalculated automatically! Get information about candidate by 
  clicking on a line in the leaderboard table.

### **Tips**

- Experiment Tracking: Runs allow you to log and track different parameters, metrics, and artifacts associated with your
  machine learning experiments. This helps in keeping a detailed record of what was done during each experiment.

- Performance Comparison: Leaderboards provide a structured way to compare the performance of different models or 
  approaches on a common task or dataset. This helps in identifying which methods are more effective.